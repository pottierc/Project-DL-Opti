{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTRUCTIONS (to read carefully and check!)\n",
    "\n",
    "- [ X ] This programming exercise is to be completed in pairs (2 students). Not less, not more.\n",
    "\n",
    "- [ X ] Fill in the name of each student in the cell below.  \n",
    "**If you do not include both names, your submission will not be graded.**\n",
    "\n",
    "- [ ] You must submit a single file per group on Moodle (this Jupyter notebook completed with your answers). Only one submission per group.  \n",
    "**If you send more than one file, your submission will not be graded.**\n",
    "\n",
    "- [ ] Your submission file must be named `APM52445_lastname1_lastname2.ipynb` (e.g., Aymeric Dieuleveut and Adrien Taylor will submit the file `APM52445_dieuleveut_taylor.ipynb`).  \n",
    "**If you do not properly name your file, your submission will not be graded.**\n",
    "\n",
    "- [ ] You Jupyter notebook must run on the free version of Google Collab using a T4 runtime environment. In this conditions, each cell should not take more than 4 minutes to run.  \n",
    "**If your code does not run or any cell takes longer than 4 minutes to complete, your submission will not be graded.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1 = \"Victor Soto\"\n",
    "student2 = \"Cl√©ment Pottier\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust learning on the CIFAR-10 dataset\n",
    "\n",
    "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) provides 60000 32x32-pixel images, classified into 10 categories.\n",
    "\n",
    "Your goals in this lab will be to:\n",
    "1. Adapt and fine-tune a pretrained model to work on CIFAR-10\n",
    "2. Evaluate the nominal and adversarial robustness of the fine-tuned model\n",
    "3. Train a robust model on CIFAR-10\n",
    "4. Evaluate the nominal and adversarial robustness of the robust model\n",
    "\n",
    "It is recommended to use a GPU to run this notebook. This will drastically speed up computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10446,
     "status": "ok",
     "timestamp": 1740042629246,
     "user": {
      "displayName": "Luiz Chamon",
      "userId": "05727507017216075476"
     },
     "user_tz": -60
    },
    "id": "401TnnoOMrgo",
    "outputId": "15958e57-ba03-4edb-e65f-60e23925ca76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, running on CPU.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# If you have a GPU installed and configured correctly, this code will allow the use of gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    batch_size = 128\n",
    "    pin_memory = True\n",
    "else:\n",
    "    print(\"No GPU found, running on CPU.\")\n",
    "    batch_size = 32\n",
    "    pin_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading CIFAR-10 dataset from torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4223,
     "status": "ok",
     "timestamp": 1740042633524,
     "user": {
      "displayName": "Luiz Chamon",
      "userId": "05727507017216075476"
     },
     "user_tz": -60
    },
    "id": "J9DQs7q-Mrgz",
    "outputId": "eb7b01de-6f06-4313-f607-985b94e8ff3e"
   },
   "outputs": [],
   "source": [
    "# Mean, standard deviation, and labels\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "CIFAR10_LABELS = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "\n",
    "# Define transforms for the training and testing datasets\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=multiprocessing.cpu_count(),\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=multiprocessing.cpu_count(),\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at a pretrained ResNet-18\n",
    "\n",
    "`torchvision` provides a series of models and weights that have been (pre)trained on ImageNet1K, a subset of the ImageNet dataset. These pretrained models can be obtained by using the `weights` parameters as in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. How many outputs does `model` have?**  \n",
    "*(you must read out the answer directly from `model` rather writing an explicit number)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1000 features\n"
     ]
    }
   ],
   "source": [
    "model_outputs = model.fc.out_features\n",
    "print(f\"The model has {model_outputs} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. That's too many for CIFAR-10! Change the last layer to fit the CIFAR-10 dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Modify the last fully connected layer to have 10 output features\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "\n",
    "# Verify the change\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Complete the function `nominal_accuracy` that takes a `model` and computes its accuracy on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of pretrained model: 6.91%\n"
     ]
    }
   ],
   "source": [
    "def nominal_accuracy(model):\n",
    "    model.eval()\n",
    "    test_acc = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            test_acc += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate accuracy as a percentage\n",
    "    test_acc = test_acc / total\n",
    "    \n",
    "    return test_acc\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "acc_pretrained = nominal_accuracy(model)\n",
    "print(f\"Test accuracy of pretrained model: {100*acc_pretrained:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Why is the accuracy so low?**  \n",
    "*(Tip: check the documentation of `torchvision.models.resnet18`)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre-trained Weights:\n",
    "    - The pre-trained resnet18 model from torchvision is trained on the ImageNet dataset, which has 1,000 classes and is significantly different from the CIFAR-10 dataset, which has only 10 classes.\n",
    "    - The features learned from ImageNet may not directly transfer well to CIFAR-10 without fine-tuning.\n",
    "- Difference in Datasets:\n",
    "    - ImageNet: Contains high-resolution images with a variety of objects and scenes.\n",
    "    - CIFAR-10: Contains low-resolution (32x32) images of objects and animals in a more constrained setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the pretrained ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Train `model` on CIFAR-10 using SGD with step size $10^{-2}$ for 3 epochs.**  \n",
    "*(this process of training for very few iterations a pretrained model is often called \"fine tuning\")*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "model.train()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# YOUR CODE OR ANSWER HERE\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_finetuned = nominal_accuracy(model)\n",
    "print(f\"Test accuracy of fine-tuned model: {100*acc_finetuned:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the robustness of the fine-tuned ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our fine-tuned `model` now has good accuracy on CIFAR-10, it turns out to be extremely sensitive to input perturbations. In fact, our `model` can be fooled by small perturbations of the CIFAR-10 images that are *invisible to the naked eye*. That's exactly what we're gonna show next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that, the idea is to essentially turn everything we've done so far on its head. Up until now, we have fixed the inputs (the data points $x_n$) and tried to **find the parameters $\\theta$ that minimize the loss**, i.e.,\n",
    "$$\n",
    "\\text{(PI):} \\quad \\min_{\\theta}\\ \\frac{1}{N} \\sum_{n=1}^N \\text{Loss}(f_\\theta(x_n),y_n)\n",
    "    \\text{,}\n",
    "$$\n",
    "where $f_\\theta$ is our model. We did that by using, for instance, stochastic gradient descent as in\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_{\\theta} \\text{Loss}(f_\\theta(x_n),y_n)\n",
    "    \\text{,} \\quad n \\sim \\text{Uniform}([1,N])\n",
    "    \\text{.}\n",
    "$$\n",
    "\n",
    "Instead, we are now going to fix the parameters $\\theta$ and some input $(x^\\prime,y^\\prime)$ and **find an input $x$ close to $x^\\prime$ that maximizes the loss**, i.e.,\n",
    "$$\n",
    "\\text{(PII):} \\quad \\max_{\\Vert x - x^\\prime \\Vert_{\\infty} \\leq \\epsilon}\\ \\text{Loss}(f_\\theta(x),y^\\prime)\n",
    "    \\text{.}\n",
    "$$\n",
    "By maximizing the loss with respect to the correct class $y^\\prime$, we are finding an image $x$ that our model does not classify as $y^\\prime$. But if that image is so close to $x^\\prime$ that we (humans) cannot tell them apart, then clearly our model has made a mistake. To achieve that, (PII) only optimizes over inputs $x$ such that $\\Vert x - x^\\prime \\Vert_{\\infty} \\leq \\epsilon$. Recall that $\\Vert z \\Vert_\\infty = \\max_i \\vert z_i \\vert$. In words, this means that the value of each pixel of $x$ cannot be more than $\\epsilon$ away from the original image $x^\\prime$. So $\\epsilon$ is the \"strength of our attack.\"\n",
    "\n",
    "Typically, we only care about very small $\\epsilon$ (say 3% of the maximum pixel value). Why? Because otherwise (PII) would just replace every image in CIFAR-10 by a picture of one of your instructors, which would completely confuse our model since none of us are part of CIFAR-10! (what would our labels even be?! Don't answer that...). No, we want $x$ and $x^\\prime$ to look almost the same to us (humans) so that our model is actually making a mistake by not assigning the class $y^\\prime$ to $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now develop an algorithm to solve (PII). To do so, observe 4 important differences between (PI) and (PII):\n",
    "\n",
    "1. (PII) optimizes over $x$ and not $\\theta$, i.e., we need to use the gradient with respect to the input $\\nabla_x \\text{Loss}(f_\\theta(x),y^\\prime)$ and **not** the gradient with respect to the parameters $\\nabla_{\\theta} \\text{Loss}(f_\\theta(x),y^\\prime)$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Complete the function `grad_input` which computes the gradient of the loss with respect to the input (and not the parameters!).**  \n",
    "*Tip: PyTorch will only compute gradients for any `torch.tensor` that has `requires_grad=True`.*  \n",
    "*Tip: Check the shape of your output: it has to match `inputs.shape`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_input(model, inputs, targets, criterion):\n",
    "    \n",
    "    #\n",
    "    #\n",
    "    # YOUR CODE OR ANSWER HERE\n",
    "    #\n",
    "    #\n",
    "    \n",
    "\n",
    "    return grad_wrt_inputs\n",
    "\n",
    "\n",
    "max_grad = 0.0\n",
    "for inputs, targets in train_loader:\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    grad = grad_input(model, inputs, targets, torch.nn.CrossEntropyLoss())\n",
    "    max_grad = max(max_grad, grad.abs().max().item())\n",
    "\n",
    "print(f\"The maximum absolute value of the gradient is {max_grad:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (PII) typically uses a pretrained network, so $\\nabla_x \\text{Loss}(f_\\theta(x),y^\\prime)$ is very small (see above). To overcome that, we use the *sign of the gradient* in the update and **not** than the gradient itself;\n",
    "3. (PII) *maximizes* rather than *minimizes* the loss, i.e., we need to do gradient *ascent* and **not** gradient *descent*;\n",
    "4. (PII) only considers inputs $x$ close to the reference $x^\\prime$. In fact, the value of the each pixel of $x$ cannot be more than $\\epsilon$ away from the value of the same pixel in $x^\\prime$. To ensure that our updates stay within $\\epsilon$ of $x^\\prime$, we use a *projection* after each update. In this case, the projection reduces to clipping as in\n",
    "    $$\n",
    "    \\text{Projection}(x) = x^\\prime + \\big[ x - x^\\prime \\big]_{-\\epsilon}^{\\epsilon}\n",
    "    \\text{,}\n",
    "    $$\n",
    "    where $\\big[ z \\big]_{-\\epsilon}^{\\epsilon}$ \"clips\" (or \"clamps\") $z$ to $[-\\epsilon,\\epsilon]$.\n",
    "    In math, we write that as $\\big[ z \\big]_{-\\epsilon}^{\\epsilon} = \\min(\\max(z,-\\epsilon),\\epsilon)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Use these four observations to modify the GD algorithm to solve (PII).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#\n",
    "#\n",
    "# YOUR CODE OR ANSWER HERE\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Complete the function `grad_attack` that uses this algorithm to find modifications of `input` ($x^\\prime$) that maximize the loss (`criterion`) of `model` ($f_\\theta$).**  \n",
    "1. initialize $x_0$ randomly in $\\Vert x - x^\\prime \\Vert_{\\infty} \\leq \\epsilon$\n",
    "1. compute the gradient of the loss (cross-entropy) with respect to the input at $x_k$\n",
    "1. update $x_k$ using the signed gradient ascent with step size `eta`\n",
    "1. project the result to get $x_{k+1}$\n",
    "1. repeat for `iterations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740043579967,
     "user": {
      "displayName": "Luiz Chamon",
      "userId": "05727507017216075476"
     },
     "user_tz": -60
    },
    "id": "53id26_uN_7f"
   },
   "outputs": [],
   "source": [
    "def grad_attack(model, inputs, targets, epsilon, eta, iterations):\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    #\n",
    "    #\n",
    "    # YOUR CODE OR ANSWER HERE\n",
    "    #\n",
    "    #\n",
    "    \n",
    "\n",
    "    return mod_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Complete the function `robustness` that takes a `model` and computes its accuracy after modifying the input with `grad_attack`.**  \n",
    "*Since we modify normalized inputs, we need to consider values of `epsilon`, the magnitude of our modifications, that are proportional to the range of the normalized CIFAR-10 images.*  \n",
    "*Set `epsilon` to 3% of that range  ($0.03 \\times 2.5$), `eta` = `epsilon`/20, and `iterations`=50).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustness(model):\n",
    "    model.eval()\n",
    "    robust_acc = 0\n",
    "\n",
    "    \n",
    "    #\n",
    "    #\n",
    "    # YOUR CODE OR ANSWER HERE\n",
    "    #\n",
    "    #\n",
    "    \n",
    "\n",
    "    return robust_acc\n",
    "\n",
    "\n",
    "robust_acc_finetuned = robustness(model)\n",
    "print(f\"Robustness of pretrained model: {100*robust_acc_finetuned:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. What happened to the accuracy of our ResNet-18? How come it went from over 70% to worst than a random classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#\n",
    "#\n",
    "# YOUR CODE OR ANSWER HERE\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 3612,
     "status": "ok",
     "timestamp": 1740044314241,
     "user": {
      "displayName": "Luiz Chamon",
      "userId": "05727507017216075476"
     },
     "user_tz": -60
    },
    "id": "6TEcp6pib9Db",
    "outputId": "e98d2127-70c6-4c70-99a6-ca88fd032402"
   },
   "outputs": [],
   "source": [
    "def unnormalize(x):\n",
    "    std = torch.tensor([[CIFAR10_STD]]).reshape((3, 1, 1))\n",
    "    mean = torch.tensor([[CIFAR10_MEAN]]).reshape(3, 1, 1)\n",
    "    return torch.clamp(x * std + mean, 0, 1)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 7, figsize=(17, 6))\n",
    "\n",
    "for n, idx in enumerate(np.random.randint(0, len(test_dataset), 7)):\n",
    "    input, target = test_dataset[idx]\n",
    "    input, target = input.to(device), torch.tensor([target], device=device)\n",
    "\n",
    "    mod_input = grad_attack(\n",
    "        model, input.unsqueeze(0), target, epsilon=0.03 * 2.5, eta=0.03 * 2.5 / 20, iterations=50\n",
    "    )\n",
    "\n",
    "    _, pred = model(input.unsqueeze(0)).max(1)\n",
    "    _, mod_pred = model(mod_input).max(1)\n",
    "\n",
    "    input_img = unnormalize(input.to(\"cpu\")).movedim(0, 2)\n",
    "    axes[0, n].imshow(input_img)\n",
    "    _ = axes[0, n].set_title(f\"Predicted: {CIFAR10_LABELS[pred]}\\n(True: {CIFAR10_LABELS[target]})\")\n",
    "\n",
    "    mod_input_img = unnormalize(mod_input.to(\"cpu\").squeeze()).movedim(0, 2)\n",
    "    axes[1, n].imshow(mod_input_img)\n",
    "    _ = axes[1, n].set_title(f\"Predicted: {CIFAR10_LABELS[mod_pred]}\")\n",
    "\n",
    "    difference = (input_img - mod_input_img).abs()\n",
    "    axes[2, n].imshow(difference / difference.max())\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_axis_on()\n",
    "axes[0, 0].xaxis.set_visible(False)\n",
    "axes[0, 0].tick_params(left=False, labelleft=False)\n",
    "_ = axes[0, 0].set_ylabel(\"Original images\")\n",
    "\n",
    "axes[1, 0].set_axis_on()\n",
    "axes[1, 0].xaxis.set_visible(False)\n",
    "axes[1, 0].tick_params(left=False, labelleft=False)\n",
    "_ = axes[1, 0].set_ylabel(\"Perturbed images\")\n",
    "\n",
    "axes[2, 0].set_axis_on()\n",
    "axes[2, 0].xaxis.set_visible(False)\n",
    "axes[2, 0].tick_params(left=False, labelleft=False)\n",
    "_ = axes[2, 0].set_ylabel(\"Difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Do the images look different? Do their differences look like anything? How come our model is now consistently wrong on images it used to get right?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#\n",
    "#\n",
    "# YOUR CODE OR ANSWER HERE\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Fine tune your (already fine tuned) CIFAR-10 model by training on modified inputs instead of clean ones. In other words, train on images after running them through `grad_attack`.**  \n",
    "*Train your model using SGD with step size $10^{-3}$ for 3 epochs.*  \n",
    "*To speed up your training, use a much weaker attack by keeping `epsilon` as before but setting `eta` = `epsilon`/3 and `iterations` = 5.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# YOUR CODE OR ANSWER HERE\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSWNPegNb9Dc",
    "outputId": "a6b75af3-b8be-4669-ee24-64f814fc1330"
   },
   "outputs": [],
   "source": [
    "acc_adv = nominal_accuracy(model)\n",
    "print(\n",
    "    f\"Test accuracy of adversarially trained model: {100*acc_adv:.2f}%\"\n",
    "    f\" (before adversarial training: {100*acc_finetuned:.2f}%)\"\n",
    ")\n",
    "\n",
    "robust_acc_adv = robustness(model)\n",
    "print(\n",
    "    f\"Robustness of adversarially trained model: {100*robust_acc_adv:.2f}%\"\n",
    "    f\" (before adversarial training: {100*robust_acc_finetuned:.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Compare the nominal accuracy (accuracy on clean samples) and robustness (accuracy on modified samples) of your model before and after adversarial training. Do you notice any patterns?**\n",
    "\n",
    "**BONUS: How would you go about dealing with it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#\n",
    "#\n",
    "# YOUR CODE OR ANSWER HERE\n",
    "#\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
